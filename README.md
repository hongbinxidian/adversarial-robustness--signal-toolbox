Based on the toolbox(adversarial-robustness--toolbox: https://github.com/Trusted-AI/adversarial-robustness-toolbox), we dpne this toolbox as the quantized methods for the paper(Faking Signals to Fool Deep Neural Networks in AMC via Few Data Points).

# Installation
Please refer to https://github.com/Trusted-AI/adversarial-robustness-toolbox

# Data
RML2016.10a_dict.pkl: https://www.deepsig.ai/datasets

# Run
python examples/signal_get_started_keras.py

# Citation
@ARTICLE{9520398,
  author={Ma, Hongbin and Yang, Shuyuan and He, Guangjun and Wu, Ruowu and Hao, Xiaojun and Li, Tingpeng and Feng, Zhixi},
  journal={IEEE Access}, 
  title={Faking Signals to Fool Deep Neural Networks in AMC via Few Data Points}, 
  year={2021},
  volume={},
  number={},
  pages={1-1},
  doi={10.1109/ACCESS.2021.3106704}}

